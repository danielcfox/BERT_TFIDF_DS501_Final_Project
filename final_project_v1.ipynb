{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shiquan/opt/anaconda3/envs/ds501/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before removing null text ones:  (9680, 5)\n",
      "No dupliate title in the data.\n",
      "4522 9680\n",
      "Shape after removing null text ones:  (9677, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>1987</td>\n",
       "      <td>Bit-Serial Neural Networks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>1987</td>\n",
       "      <td>Connectivity Versus Entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>1987</td>\n",
       "      <td>The Hopfield Model with Multi-Level Neurons</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>1987</td>\n",
       "      <td>How Neural Nets Work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>1987</td>\n",
       "      <td>Spatial Organization of Neural Networks: A Pro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id  year                                              title  \\\n",
       "0         27  1987                         Bit-Serial Neural Networks   \n",
       "1         63  1987                        Connectivity Versus Entropy   \n",
       "2         60  1987        The Hopfield Model with Multi-Level Neurons   \n",
       "3         59  1987                               How Neural Nets Work   \n",
       "4         69  1987  Spatial Organization of Neural Networks: A Pro...   \n",
       "\n",
       "  abstract                                          full_text  id  \n",
       "0      NaN  573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...   1  \n",
       "1      NaN  1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...   2  \n",
       "2      NaN  278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...   3  \n",
       "3      NaN  442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...   4  \n",
       "4      NaN  740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...   5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from context_doc_embed import *\n",
    "\n",
    "data_dir = \"/Users/Shiquan/Documents/WPI Courses/Intro of DS/Project/Datasets/NIPS 2019\"\n",
    "# author_path = os.path.join(data_dir, \"authors.csv\")\n",
    "paper_path = os.path.join(data_dir, \"papers.csv\")\n",
    "# author_df = pd.read_csv(author_path)\n",
    "paper_df = pd.read_csv(paper_path)\n",
    "print(\"Shape before removing null text ones: \", paper_df.shape)  # author_df.shape,\n",
    "if len(paper_df) == paper_df.title.nunique():\n",
    "    print(\"No dupliate title in the data.\")\n",
    "    paper_df[\"id\"] = range(1, len(paper_df) + 1)\n",
    "else:\n",
    "    print(\"Dupliate titles in the data!\")\n",
    "print(paper_df.source_id.nunique(), paper_df.title.nunique())\n",
    "\n",
    "paper_df.dropna(subset=[\"full_text\"], inplace=True)\n",
    "print(\"Shape after removing null text ones: \", paper_df.shape)\n",
    "\n",
    "paper_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1825, 410, 4508, 4013, 3658]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample some papers as our target papers\n",
    "\n",
    "target_paper_num = 5\n",
    "\n",
    "random.seed(42)\n",
    "target_ids = random.sample(paper_df['id'].tolist(), target_paper_num)\n",
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Google Colab for getting embeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "import pickle\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "save_dir = '/content/drive/MyDrive/Colab/Colab data/NIPS 2019'\n",
    "num_per_file = 1000\n",
    "\n",
    "paper_list = []\n",
    "\n",
    "st_time = time()\n",
    "\n",
    "for i, a_paper in tqdm(paper_df.iterrows()):\n",
    "    paper_info = dict()\n",
    "    paper_info['id'] = a_paper['id']\n",
    "    paper_info['title'] = a_paper['title']\n",
    "    paper_info['sentence'] = get_norm_sentences_from_text(a_paper['full_text'])\n",
    "    paper_info['sentence_embed'] = [sbert_model.encode(x) for x in paper_info['sentence']]\n",
    "    paper_info['doc_embed'] = pd.DataFrame(paper_info['sentence_embed']).apply(np.mean, axis=0).to_numpy()\n",
    "    paper_list.append(paper_info)\n",
    "    if (i > 0) & (i % num_per_file == 0)| (i == max(paper_df.index)):\n",
    "        file_index = i // num_per_file if i != max(paper_df.index) else ((i // num_per_file) +1)\n",
    "        paper_embeddings = {\"paper\": paper_list}\n",
    "        processed_paper_file_name = f\"processed_nips_paper_{file_index}.pickle\"\n",
    "        with open(os.path.join(save_dir, processed_paper_file_name), \"wb\") as pkl:\n",
    "            pickle.dump(paper_embeddings, pkl)\n",
    "        paper_list = []\n",
    "        \n",
    "## It takes 5 hours on getting the embeddings\n",
    "\n",
    "# paper_embeddings = {\"paper\": paper_list}\n",
    "\n",
    "\n",
    "\n",
    "# processed_paper_file_name = \"processed_nips_paper.json\"\n",
    "# with open(processed_paper_file_name, \"w\") as fp:\n",
    "#     json.dump(paper_embeddings, fp, indent=2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_dir = \"/Users/Shiquan/Library/CloudStorage/GoogleDrive-s.quanhe@gmail.com/My Drive/Colab/Colab data/NIPS 2019\"\n",
    "file_list = [\"processed_nips_paper_\" + str(x) + \".pickle\" for x in range(1, 11)]\n",
    "file_list = [os.path.join(file_dir, x) for x in file_list]\n",
    "\n",
    "save_dir2 = '/Users/Shiquan/Documents/WPI Courses/Intro of DS/Project/Datasets/NIPS 2019'\n",
    "\n",
    "merge_pickles(file_path_list=file_list, \n",
    "              new_file_path=os.path.join(save_dir2, 'sentence_doc_embeddings.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_dir2, 'sentence_doc_embeddings.pickle'), 'rb') as pkl:\n",
    "            sent_doc_embed = pickle.load(pkl)\n",
    "\n",
    "print(len(sent_doc_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a smaller file with only id, title and doc embeddings\n",
    "\n",
    "doc_embed_list = []\n",
    "\n",
    "for one_doc in sent_doc_embed:\n",
    "    del one_doc['sentence']\n",
    "    del one_doc['sentence_embed']\n",
    "    doc_embed_list.append(one_doc)\n",
    "\n",
    "\n",
    "with open(os.path.join(save_dir2, 'doc_embeddings.pickle'), \"wb\") as pkl:\n",
    "    pickle.dump(doc_embed_list, pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_dir2, 'doc_embeddings.pickle'), 'rb') as pkl:\n",
    "    doc_embed_list = pickle.load(pkl)\n",
    "\n",
    "doc_embed_df = pd.DataFrame(doc_embed_list)\n",
    "\n",
    "print(doc_embed_df.shape)\n",
    "doc_embed_df.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = target_ids[0]\n",
    "# non_null_embed_indexes = [x for x in range(len(doc_embed_list)) if x!= 6104]\n",
    "\n",
    "print(doc_embed_df.loc[doc_embed_df.id==target_id, 'title'])\n",
    "get_most_similar_docs_from_list(\n",
    "        target_id=target_id, \n",
    "        embed_list=doc_embed_list)\n",
    "\n",
    "## 6104 has null full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = target_ids[1]\n",
    "\n",
    "print(doc_embed_df.loc[doc_embed_df.id==target_id, 'title'])\n",
    "get_most_similar_docs_from_list(\n",
    "        target_id=target_id, \n",
    "        embed_list=doc_embed_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = target_ids[2]\n",
    "\n",
    "print(doc_embed_df.loc[doc_embed_df.id==target_id, 'title'])\n",
    "get_most_similar_docs_from_list(\n",
    "        target_id=target_id, \n",
    "        embed_list=doc_embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = target_ids[3]\n",
    "\n",
    "print(doc_embed_df.loc[doc_embed_df.id==target_id, 'title'])\n",
    "get_most_similar_docs_from_list(\n",
    "        target_id=target_id, \n",
    "        embed_list=doc_embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = target_ids[4]\n",
    "\n",
    "print(doc_embed_df.loc[doc_embed_df.id==target_id, 'title'])\n",
    "get_most_similar_docs_from_list(\n",
    "        target_id=target_id, \n",
    "        embed_list=doc_embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_title1 = \"Computing with Finite and Infinite Networks\"\n",
    "target_text1 = paper_df.loc[paper_df['title']==target_title1, 'full_text'].item()\n",
    "print(get_rouge_metrics(\n",
    "    candi_text=bert_found1_df.iloc[0]['full_text'], reference=target_text1))\n",
    "\n",
    "print(get_mean_rouge(candidate_df=bert_found1_df, reference=target_text1))\n",
    "\n",
    "print(bert_found1_df.iloc[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = target_ids[0]\n",
    "# non_null_embed_indexes = [x for x in range(len(doc_embed_list)) if x!= 6104]\n",
    "\n",
    "print(doc_embed_df.loc[doc_embed_df.id==target_id, 'title'])\n",
    "bert_found1_df = get_most_similar_docs_from_list(\n",
    "        target_id=target_id, \n",
    "        embed_list=doc_embed_list)\n",
    "bert_found1_df = bert_found1_df.merge(paper_df[['id', 'full_text']], \n",
    "                                      on='id', how='left')\n",
    "bert_found1_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = target_ids[1]\n",
    "\n",
    "print(doc_embed_df.loc[doc_embed_df.id==target_id, 'title'])\n",
    "bert_found2_df = get_most_similar_docs_from_list(\n",
    "        target_id=target_id, \n",
    "        embed_list=doc_embed_list)\n",
    "\n",
    "bert_found2_df = bert_found2_df.merge(paper_df[['id', 'full_text']], \n",
    "                                      on='id', how='left')\n",
    "bert_found2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_title2 = 'Integrated Segmentation and Recognition of Hand-Printed Numerals'\n",
    "target_text2 = paper_df.loc[paper_df['title']==target_title2, 'full_text'].item()\n",
    "print(get_rouge_metrics(\n",
    "    candi_text=bert_found2_df.iloc[0]['full_text'], reference=target_text2))\n",
    "\n",
    "print(get_mean_rouge(candidate_df=bert_found2_df, reference=target_text2))\n",
    "\n",
    "print(bert_found2_df.iloc[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rouge.rouge.Rouge"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_id = target_ids[2]\n",
    "\n",
    "print(doc_embed_df.loc[doc_embed_df.id==target_id, 'title'])\n",
    "bert_found3_df = get_most_similar_docs_from_list(\n",
    "        target_id=target_id, \n",
    "        embed_list=doc_embed_list)\n",
    "\n",
    "bert_found3_df = bert_found3_df.merge(paper_df[['id', 'full_text']], \n",
    "                                      on='id', how='left')\n",
    "bert_found3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.2806236080178174,\n",
       "  'p': 0.16578947368421051,\n",
       "  'f': 0.2084367198966123},\n",
       " 'rouge-2': {'r': 0.05944055944055944,\n",
       "  'p': 0.03426432479124676,\n",
       "  'f': 0.04347031499563712},\n",
       " 'rouge-l': {'r': 0.2639198218262806,\n",
       "  'p': 0.15592105263157896,\n",
       "  'f': 0.19602977200579344}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_title3 = 'Accelerated Training for Matrix-norm Regularization: A Boosting Approach'\n",
    "target_text3 = paper_df.loc[paper_df['title']==target_title3, 'full_text'].item()\n",
    "print(get_rouge_metrics(\n",
    "    candi_text=bert_found3_df.iloc[0]['full_text'], reference=target_text3))\n",
    "\n",
    "print(get_mean_rouge(candidate_df=bert_found3_df, reference=target_text3))\n",
    "\n",
    "print(bert_found3_df.iloc[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2084367198966123"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_id = target_ids[3]\n",
    "\n",
    "print(doc_embed_df.loc[doc_embed_df.id==target_id, 'title'])\n",
    "bert_found4_df = get_most_similar_docs_from_list(\n",
    "        target_id=target_id, \n",
    "        embed_list=doc_embed_list)\n",
    "\n",
    "bert_found4_df = bert_found4_df.merge(paper_df[['id', 'full_text']], \n",
    "                                      on='id', how='left')\n",
    "bert_found4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.2806236080178174,\n",
       "   'p': 0.16578947368421051,\n",
       "   'f': 0.2084367198966123},\n",
       "  'rouge-2': {'r': 0.05944055944055944,\n",
       "   'p': 0.03426432479124676,\n",
       "   'f': 0.04347031499563712},\n",
       "  'rouge-l': {'r': 0.2639198218262806,\n",
       "   'p': 0.15592105263157896,\n",
       "   'f': 0.19602977200579344}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_title4 = 'Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models'\n",
    "target_text4 = paper_df.loc[paper_df['title']==target_title4, 'full_text'].item()\n",
    "print(get_rouge_metrics(\n",
    "    candi_text=bert_found4_df.iloc[0]['full_text'], reference=target_text4))\n",
    "\n",
    "print(get_mean_rouge(candidate_df=bert_found4_df, reference=target_text4))\n",
    "\n",
    "print(bert_found4_df.iloc[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_id = target_ids[4]\n",
    "\n",
    "print(doc_embed_df.loc[doc_embed_df.id==target_id, 'title'])\n",
    "bert_found5_df = get_most_similar_docs_from_list(\n",
    "        target_id=target_id, \n",
    "        embed_list=doc_embed_list)\n",
    "\n",
    "bert_found5_df = bert_found5_df.merge(paper_df[['id', 'full_text']], \n",
    "                                      on='id', how='left')\n",
    "bert_found5_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the ROUGEs from top 10 relevant paper for a target paper\n",
    "\n",
    "bert_found_df = bert_found5_df\n",
    "target_text = target_text5\n",
    "rouge_lst = [get_rouge_metrics(candi_text=bert_found_df.iloc[i]['full_text'], reference=target_text) for i in range(0, 10)]\n",
    "\n",
    "for i, a_rouge in enumerate(rouge_lst):\n",
    "    print(bert_found_df.iloc[i]['title'])\n",
    "    print(a_rouge[0]['rouge-1']['f'], a_rouge[0]['rouge-l']['f'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the keywords for all the doc in NIPS data\n",
    "from tqdm import tqdm\n",
    "\n",
    "paper_list = []\n",
    "consider_top_n = 50\n",
    "save_dir2 = '/Users/Shiquan/Documents/WPI Courses/Intro of DS/Project/Datasets/NIPS 2019'\n",
    "\n",
    "for i, a_paper in tqdm(paper_df.iterrows()):\n",
    "    paper_info = dict()\n",
    "    paper_info['id'] = a_paper['id']\n",
    "    paper_info['title'] = a_paper['title']\n",
    "    paper_info['full_text'] = a_paper['full_text']\n",
    "    keyword_info = get_top_keywords(text=a_paper['full_text'], top_n=consider_top_n)\n",
    "    paper_info['key_words'] = keyword_info['key_word']\n",
    "    paper_info['rank_scores'] = keyword_info['rank_score']\n",
    "    paper_info['doc_len'] = get_word_count(a_paper['full_text'])\n",
    "    paper_list.append(paper_info)\n",
    "\n",
    "paper_keywords = {\"paper\": paper_list}\n",
    "with open(os.path.join(save_dir2, \"nips_paper_key_words.pickle\"), \"wb\") as pkl:\n",
    "    pickle.dump(paper_keywords, pkl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_dir2, \"nips_paper_key_words.pickle\"), \"wb\") as pkl:\n",
    "    pickle.dump(paper_keywords, pkl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(save_dir2, \"nips_paper_key_words.pickle\"), 'rb') as pkl:\n",
    "    paper_keywords = pickle.load(pkl)\n",
    "\n",
    "paper_keywords_list = paper_keywords['paper']\n",
    "print(len(paper_keywords_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id=target_ids[0]\n",
    "tr_found1_df = get_most_similar_docs_from_list_textrank(\n",
    "    target_id=target_id,\n",
    "    keyword_info_list=paper_keywords_list,\n",
    "    target_text= None,\n",
    "    top_n= 10,\n",
    "    top_n_keywords= 20,\n",
    ")\n",
    "tr_found1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_title = \"Computing with Finite and Infinite Networks\"\n",
    "target_text = paper_df.loc[paper_df['title']==target_title, 'full_text'].item()\n",
    "print(get_rouge_metrics(\n",
    "    candi_text=tr_found1_df.iloc[0]['full_text'], reference=target_text))\n",
    "print(get_mean_rouge(candidate_df=tr_found1_df, reference=target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id=target_ids[1]\n",
    "tr_found2_df = get_most_similar_docs_from_list_textrank(\n",
    "    target_id=target_id,\n",
    "    keyword_info_list=paper_keywords_list,\n",
    "    target_text= None,\n",
    "    top_n= 10,\n",
    "    top_n_keywords= 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_title2 = \"Integrated Segmentation and Recognition of Hand-Printed Numerals\"\n",
    "target_text2 = paper_df.loc[paper_df['title']==target_title2, 'full_text'].item()\n",
    "print(get_rouge_metrics(\n",
    "    candi_text=tr_found2_df.iloc[0]['full_text'], reference=target_text2))\n",
    "\n",
    "print(get_mean_rouge(candidate_df=tr_found2_df, reference=target_text2))\n",
    "\n",
    "print(tr_found2_df.iloc[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id=target_ids[2]\n",
    "tr_found3_df = get_most_similar_docs_from_list_textrank(\n",
    "    target_id=target_id,\n",
    "    keyword_info_list=paper_keywords_list,\n",
    "    target_text= None,\n",
    "    top_n= 10,\n",
    "    top_n_keywords= 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_title3 = \"Accelerated Training for Matrix-norm Regularization: A Boosting Approach\"\n",
    "target_text3 = paper_df.loc[paper_df['title']==target_title3, 'full_text'].item()\n",
    "print(get_rouge_metrics(\n",
    "    candi_text=tr_found3_df.iloc[0]['full_text'], reference=target_text3))\n",
    "\n",
    "print(get_mean_rouge(candidate_df=tr_found3_df, reference=target_text3))\n",
    "\n",
    "print(tr_found3_df.iloc[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id=target_ids[3]\n",
    "tr_found4_df = get_most_similar_docs_from_list_textrank(\n",
    "    target_id=target_id,\n",
    "    keyword_info_list=paper_keywords_list,\n",
    "    target_text= None,\n",
    "    top_n= 10,\n",
    "    top_n_keywords= 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_title4 = \"Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models\"\n",
    "target_text4 = paper_df.loc[paper_df['title']==target_title4, 'full_text'].item()\n",
    "print(get_rouge_metrics(\n",
    "    candi_text=tr_found4_df.iloc[0]['full_text'], reference=target_text4))\n",
    "\n",
    "print(get_mean_rouge(candidate_df=tr_found4_df, reference=target_text4))\n",
    "\n",
    "print(tr_found4_df.iloc[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id=target_ids[4]\n",
    "tr_found5_df=get_most_similar_docs_from_list_textrank(\n",
    "    target_id=target_id,\n",
    "    keyword_info_list=paper_keywords_list,\n",
    "    target_text= None,\n",
    "    top_n= 10,\n",
    "    top_n_keywords= 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_title5 = \"Modeling Social Annotation Data with Content Relevance using a Topic Model\"\n",
    "target_text5 = paper_df.loc[paper_df['title']==target_title5, 'full_text'].item()\n",
    "print(get_rouge_metrics(\n",
    "    candi_text=tr_found5_df.iloc[0]['full_text'], reference=target_text5))\n",
    "\n",
    "print(get_mean_rouge(candidate_df=tr_found5_df, reference=target_text5))\n",
    "\n",
    "print(tr_found5_df.iloc[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d982f7d16c8ec1c74085bc06ce65ef23161d9b6506c5213d844174c44a1129c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
